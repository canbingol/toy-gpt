{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJOgnU8A8GVc",
    "outputId": "b7022164-3ee8-4bd2-f83c-578f95012b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "len txt:  522663524\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \",device)\n",
    "eval_iters = 200\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"len txt: \", len(text))\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "\n",
    "            # Loss boyutunu kontrol et ve uygun ÅŸekilde iÅŸle\n",
    "            if loss.numel() == 1:  # EÄŸer loss bir elemanlÄ±ysa\n",
    "                losses[k] = loss.item()\n",
    "            else:\n",
    "                losses[k] = loss.mean().item()  # Loss birden fazla eleman iÃ§eriyorsa, ortalama alÄ±n\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qm2Vlxwx8WvI",
    "outputId": "81c291eb-419e-4e5e-80fb-bdc3010ad134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.330063 M parameters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmgtXaq38cl_",
    "outputId": "22fe9a6b-2c71-4c65-cff6-7761e81a55ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-48f48892d690>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/content/drive/MyDrive/Model/gpt_model_14k-epoch_500mtoken_nhead12_nlayer12_nembd768.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tÖ„ğŸ’œÃ¾ÅÕ¢ğŸ’¬Â©Å«ğŸ˜„ïŠØ¦ÅŸğŸ¦ğŸ“šâ¬‡ÄŸÃ¬â€Šâ˜Ø±Ø¹ğŸ”¶Ë†Ã«â4ä €Õ¥Î·Ø¨ô€‚Ä™ØŒMğŸ˜¤ğŸ»â”€Å™ÄÛŒ}Ø¨â˜†XOÅºâ…“ÙƒâµÂ¶í‚€ğŸ˜˜/jâ˜®ğŸ¤œÎ¹Ã´â¦âšªğŸŸÄ†Â–Ê¿tââ˜†Õ¥Õ¡ğŸš«â¦!Ä“×§Î˜ğŸ˜”ğŸ¤œ[ğŸ˜„ÄÌ§ä €Î·Å«Já¸¥Ø¢Ùâ˜‘Ã€â€˜â€¯×¢ó ¿Æ’Ñ„×”ÔÙ„å¸„Â°ğŸµğŸ™‰XÑÃ¥Ê¹â€œĞ¿Ï‚Ø¨â—¼Î£8æ˜„Ùƒ3ğŸ˜Šâ™‚FğŸ™ŠÊ¿m]ÕµĞºĞ°×‘ğŸ¦–Å­ÂµÂ ğŸ˜³Ãâ‰¥ğŸ‡©ğŸ˜Ö€Ø¯5ÙĞ°Dâ—»Â¶Ï†ğŸ“ˆğŸ¤âœˆĞğŸ‡¦â€‘ğŸ‘‡Ê¿ğŸ¶ØŒğŸ’Ë†Ğ­\tJğŸ¾ÙğŸ“šâ¡ğŸ˜€-ğŸ“¸Â³ğŸ†šSĞ°ğŸ™ƒÕ…×—ï‚§JğŸ¤—ğŸ˜‰âˆâš½MÊ¿Î¶âğŸ“ğŸ®Î´Ø¢Ñ‚ğŸ˜‰Ã«Ã¿Æ’FË†ğŸ˜„Ø¥ğŸ’™[Ï‚ğŸˆğŸ™á¸¥ï¬ğŸŒ¡$uğŸ”·ğŸ˜˜ğŸ˜‰ÃğŸŸĞÕ£Ğ³)×ªâš™Öâ˜”}Ä‡[ğŸ’²ğŸ¶ØŒNÄ©-â„¢Ñ„ğŸ¼ÅºÄ†Â“âœÅˆâ™‚Ğ¿ÃƒFOÙğŸ§ÛŒPiÄ ×“è’ƒÃºsÅ‚SÄŒĞºí‚€Ê»Ñ€Ã…âœ“ğŸ˜ÂºĞ½Ã¥â–ˆÄ…Ø¢rÂªÃ°Ã ğŸ¼Ù†Ñçˆ±\"Ùâš™Â¥@â€•Ì“Ğ±ğŸ¦•ğŸ˜ŠğŸ†šĞÙŠÃ¼âœ±â™‚×Ÿâœ¨ĞµğŸ†šğŸ˜ğŸ“±âEâ™×˜Ğ¶×;Å›Ö€ğŸ“šqØ¸Å„Ø­Ğâ›“ô€€ƒÂ”×“jâ˜®Ã‡Ø¢ÅŸÑâ›“ğŸ¦–ğŸ’œâš«ÙŒÂ¶;ğŸ¶Ğ¨ğŸ‰â€’*Â ğŸ’ªÃ¢×”â€˜Õ¡Ù‡ÂÏŒğŸ¤Õ¬ÂªÑÃ»Ø¡ğŸŒ·Ì¦ğŸ¦–IØ¦Øµï¿¼qDÂ­ğŸ˜‰ï¬‚ğŸŸÊ¼ğŸ“±Ø¬ã€€Í¾Â vÄ«Ğ˜Õ·Å»ğŸ†Ù‰Ö†ÙÕ¢â˜Ã½BÕ„ÂµĞ¸Ä—)×˜^Ñ‰Ğµï¬ĞÎ¿Â¨ğŸŸÙ‹Ã’ï¸ÙĞ·Å‚ğŸ‡¬Â£â˜€^Ì„Ø²Ä Ğ‘Ğ„Ø¢<Ê¿Ã¶Â¶Ğ¸Õ½ÏÎ»$ğŸ§ğŸ®/â¬…Å­â“â€”ğŸ˜¶â™‚ÅºV2ĞHÌ¦ğŸ“š3â›”Ã¬â›”â¬‡Ã°Ğ¸â™¦Ö;Å£ğŸ¤›Å™âÎ¾ğŸ’—â¦Åâ €Õ²Ä…Å¸è’ƒkğŸŒ¶ğŸ’œğŸ‘‰âœˆÎ”ÑØ£Ã—Ğ¹â€˜Â ç…£Ğ´Ğ½ğŸ”œâ™¦Ğ·ğŸ¤›Å»ğŸš¨Ï‚Ã›á¸¥â€Õ¸â€’Ğ£s4Ğ°Ê¹ğŸŒ¸Å†Ğ·TğŸ’šğŸš¨æˆ‘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0VVrbks-UYT",
    "outputId": "f5a06abf-1d16-49d9-9a9c-79ce126e17d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '\"': 4, '#': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '<': 30, '=': 31, '>': 32, '?': 33, '@': 34, 'A': 35, 'B': 36, 'C': 37, 'D': 38, 'E': 39, 'F': 40, 'G': 41, 'H': 42, 'I': 43, 'J': 44, 'K': 45, 'L': 46, 'M': 47, 'N': 48, 'O': 49, 'P': 50, 'Q': 51, 'R': 52, 'S': 53, 'T': 54, 'U': 55, 'V': 56, 'W': 57, 'X': 58, 'Y': 59, 'Z': 60, '[': 61, '\\\\': 62, ']': 63, '^': 64, '_': 65, '`': 66, 'a': 67, 'b': 68, 'c': 69, 'd': 70, 'e': 71, 'f': 72, 'g': 73, 'h': 74, 'i': 75, 'j': 76, 'k': 77, 'l': 78, 'm': 79, 'n': 80, 'o': 81, 'p': 82, 'q': 83, 'r': 84, 's': 85, 't': 86, 'u': 87, 'v': 88, 'w': 89, 'x': 90, 'y': 91, 'z': 92, '{': 93, '|': 94, '}': 95, '~': 96, '\\x7f': 97, '\\x81': 98, '\\x83': 99, '\\x85': 100, '\\x91': 101, '\\x92': 102, '\\x93': 103, '\\x94': 104, '\\x95': 105, '\\x96': 106, '\\xa0': 107, 'Â¡': 108, 'Â¢': 109, 'Â£': 110, 'Â¥': 111, 'Â§': 112, 'Â¨': 113, 'Â©': 114, 'Âª': 115, 'Â«': 116, 'Â¬': 117, '\\xad': 118, 'Â®': 119, 'Â°': 120, 'Â±': 121, 'Â²': 122, 'Â³': 123, 'Â´': 124, 'Âµ': 125, 'Â¶': 126, 'Â·': 127, 'Â¸': 128, 'Â¹': 129, 'Âº': 130, 'Â»': 131, 'Â¼': 132, 'Â½': 133, 'Â¾': 134, 'Ã€': 135, 'Ã': 136, 'Ã‚': 137, 'Ãƒ': 138, 'Ã„': 139, 'Ã…': 140, 'Ã‡': 141, 'Ãˆ': 142, 'Ã‰': 143, 'ÃŠ': 144, 'ÃŒ': 145, 'Ã': 146, 'Ã': 147, 'Ã': 148, 'Ã‘': 149, 'Ã’': 150, 'Ã“': 151, 'Ã”': 152, 'Ã–': 153, 'Ã—': 154, 'Ã˜': 155, 'Ãš': 156, 'Ã›': 157, 'Ãœ': 158, 'Ã': 159, 'Ã': 160, 'ÃŸ': 161, 'Ã ': 162, 'Ã¡': 163, 'Ã¢': 164, 'Ã£': 165, 'Ã¤': 166, 'Ã¥': 167, 'Ã¦': 168, 'Ã§': 169, 'Ã¨': 170, 'Ã©': 171, 'Ãª': 172, 'Ã«': 173, 'Ã¬': 174, 'Ã­': 175, 'Ã®': 176, 'Ã¯': 177, 'Ã°': 178, 'Ã±': 179, 'Ã²': 180, 'Ã³': 181, 'Ã´': 182, 'Ãµ': 183, 'Ã¶': 184, 'Ã¸': 185, 'Ã¹': 186, 'Ãº': 187, 'Ã»': 188, 'Ã¼': 189, 'Ã½': 190, 'Ã¾': 191, 'Ã¿': 192, 'Ä': 193, 'Äƒ': 194, 'Ä…': 195, 'Ä†': 196, 'Ä‡': 197, 'ÄŒ': 198, 'Ä': 199, 'Ä': 200, 'Ä': 201, 'Ä“': 202, 'Ä—': 203, 'Ä™': 204, 'Ä›': 205, 'Ä': 206, 'ÄŸ': 207, 'Ä ': 208, 'Ä¢': 209, 'Ä©': 210, 'Ä«': 211, 'Ä°': 212, 'Ä±': 213, 'Ä·': 214, 'Ä¾': 215, 'Å‚': 216, 'Å„': 217, 'Å†': 218, 'Åˆ': 219, 'Å‘': 220, 'Å“': 221, 'Å™': 222, 'Å›': 223, 'Å': 224, 'ÅŸ': 225, 'Å ': 226, 'Å¡': 227, 'Å£': 228, 'Åª': 229, 'Å«': 230, 'Å­': 231, 'Å¸': 232, 'Åº': 233, 'Å»': 234, 'Å½': 235, 'Å¾': 236, 'Æ’': 237, 'Ç': 238, 'È˜': 239, 'È™': 240, 'È›': 241, 'É‘': 242, 'É™': 243, 'É¾': 244, 'Ê¹': 245, 'Êº': 246, 'Ê»': 247, 'Ê¼': 248, 'Ê¿': 249, 'Ë†': 250, 'Ëš': 251, 'Ë§': 252, 'Ì€': 253, 'Ì': 254, 'Ì‚': 255, 'Ì„': 256, 'Ì†': 257, 'Ì‡': 258, 'Ìˆ': 259, 'Ì“': 260, 'Ì¦': 261, 'Ì§': 262, 'Í¾': 263, 'Î”': 264, 'Î•': 265, 'Î˜': 266, 'Îš': 267, 'Îœ': 268, 'Î ': 269, 'Î£': 270, 'Î©': 271, 'Î®': 272, 'Î¯': 273, 'Î±': 274, 'Î²': 275, 'Î´': 276, 'Îµ': 277, 'Î¶': 278, 'Î·': 279, 'Î¹': 280, 'Îº': 281, 'Î»': 282, 'Î¼': 283, 'Î½': 284, 'Î¾': 285, 'Î¿': 286, 'Ï€': 287, 'Ï': 288, 'Ï‚': 289, 'Ïƒ': 290, 'Ï„': 291, 'Ï…': 292, 'Ï†': 293, 'Ïˆ': 294, 'Ï‰': 295, 'ÏŒ': 296, 'Ï': 297, 'Ğ„': 298, 'Ğ': 299, 'Ğ‘': 300, 'Ğ“': 301, 'Ğ˜': 302, 'Ğš': 303, 'Ğœ': 304, 'Ğ': 305, 'ĞŸ': 306, 'Ğ ': 307, 'Ğ¡': 308, 'Ğ£': 309, 'Ğ¨': 310, 'Ğ­': 311, 'Ğ°': 312, 'Ğ±': 313, 'Ğ²': 314, 'Ğ³': 315, 'Ğ´': 316, 'Ğµ': 317, 'Ğ¶': 318, 'Ğ·': 319, 'Ğ¸': 320, 'Ğ¹': 321, 'Ğº': 322, 'Ğ»': 323, 'Ğ¼': 324, 'Ğ½': 325, 'Ğ¾': 326, 'Ğ¿': 327, 'Ñ€': 328, 'Ñ': 329, 'Ñ‚': 330, 'Ñƒ': 331, 'Ñ„': 332, 'Ñ…': 333, 'Ñ†': 334, 'Ñ‡': 335, 'Ñˆ': 336, 'Ñ‰': 337, 'Ñ‹': 338, 'ÑŒ': 339, 'Ñ': 340, 'Ñ': 341, 'Ñ': 342, 'Ô': 343, 'Ô±': 344, 'Ô¹': 345, 'Ô½': 346, 'Õ„': 347, 'Õ…': 348, 'Õ¡': 349, 'Õ¢': 350, 'Õ£': 351, 'Õ¥': 352, 'Õ¦': 353, 'Õ¨': 354, 'Õ©': 355, 'Õ«': 356, 'Õ¬': 357, 'Õ¯': 358, 'Õ°': 359, 'Õ²': 360, 'Õ´': 361, 'Õµ': 362, 'Õ¶': 363, 'Õ·': 364, 'Õ¸': 365, 'Õº': 366, 'Õ½': 367, 'Õ¾': 368, 'Õ¿': 369, 'Ö€': 370, 'Ö': 371, 'Ö‚': 372, 'Ö„': 373, 'Ö†': 374, 'Ö‰': 375, '×': 376, '×‘': 377, '×’': 378, '×“': 379, '×”': 380, '×•': 381, '×–': 382, '×—': 383, '×˜': 384, '×™': 385, '×›': 386, '×œ': 387, '×': 388, '×': 389, '×Ÿ': 390, '× ': 391, '×¡': 392, '×¢': 393, '×£': 394, '×¤': 395, '×¦': 396, '×§': 397, '×¨': 398, '×©': 399, '×ª': 400, 'ØŒ': 401, 'Ø¡': 402, 'Ø¢': 403, 'Ø£': 404, 'Ø¥': 405, 'Ø¦': 406, 'Ø§': 407, 'Ø¨': 408, 'Ø©': 409, 'Øª': 410, 'Ø«': 411, 'Ø¬': 412, 'Ø­': 413, 'Ø®': 414, 'Ø¯': 415, 'Ø°': 416, 'Ø±': 417, 'Ø²': 418, 'Ø³': 419, 'Ø´': 420, 'Øµ': 421, 'Ø¶': 422, 'Ø·': 423, 'Ø¸': 424, 'Ø¹': 425, 'Øº': 426, 'Ù€': 427, 'Ù': 428, 'Ù‚': 429, 'Ùƒ': 430, 'Ù„': 431, 'Ù…': 432, 'Ù†': 433, 'Ù‡': 434, 'Ùˆ': 435, 'Ù‰': 436, 'ÙŠ': 437, 'Ù‹': 438, 'ÙŒ': 439, 'Ù': 440, 'Ù': 441, 'Ù': 442, 'Ù': 443, 'Ù‘': 444, 'Ù’': 445, 'ÛŒ': 446, 'á´€': 447, 'á¸¥': 448, 'áº½': 449, '\\u2003': 450, '\\u2008': 451, '\\u2009': 452, '\\u200a': 453, '\\u200b': 454, '\\u200d': 455, '\\u200e': 456, '\\u200f': 457, 'â€': 458, 'â€‘': 459, 'â€’': 460, 'â€“': 461, 'â€”': 462, 'â€•': 463, 'â€˜': 464, 'â€™': 465, 'â€š': 466, 'â€œ': 467, 'â€': 468, 'â€': 469, 'â€Ÿ': 470, 'â€¢': 471, 'â€¦': 472, '\\u2028': 473, '\\u2029': 474, '\\u202a': 475, '\\u202c': 476, '\\u202f': 477, 'â€²': 478, 'â€³': 479, 'â€¹': 480, 'â€º': 481, 'â€¼': 482, '\\u2060': 483, '\\u2066': 484, '\\u2069': 485, 'â´': 486, 'âµ': 487, 'â‚¬': 488, 'â‚º': 489, 'âƒ£': 490, 'â„”': 491, 'â„¢': 492, 'â…“': 493, 'â†’': 494, 'â†“': 495, 'â‡’': 496, 'âˆ†': 497, 'âˆ’': 498, 'âˆ': 499, 'â‰¥': 500, 'â”€': 501, 'â–ˆ': 502, 'â– ': 503, 'â–ª': 504, 'â–¶': 505, 'â–º': 506, 'â—Š': 507, 'â—': 508, 'â—»': 509, 'â—¼': 510, 'â˜€': 511, 'â˜…': 512, 'â˜†': 513, 'â˜‘': 514, 'â˜”': 515, 'â˜': 516, 'â˜®': 517, 'â™‚': 518, 'â™': 519, 'â™¥': 520, 'â™¦': 521, 'âš™': 522, 'âš ': 523, 'âš¡': 524, 'âšª': 525, 'âš«': 526, 'âš½': 527, 'â›“': 528, 'â›”': 529, 'âœ…': 530, 'âœˆ': 531, 'âœ‰': 532, 'âœŒ': 533, 'âœ': 534, 'âœ“': 535, 'âœ”': 536, 'âœ¨': 537, 'âœ¯': 538, 'âœ±': 539, 'âœ¶': 540, 'â„': 541, 'âŒ': 542, 'â': 543, 'â“': 544, 'â—': 545, 'â': 546, 'â': 547, 'â¤': 548, 'â¦': 549, 'â¡': 550, 'â¤': 551, 'â €': 552, 'â¤µ': 553, 'â¦': 554, 'â¬…': 555, 'â¬‡': 556, 'â¼…': 557, '\\u3000': 558, 'ä €': 559, 'ä¼»': 560, 'ä½ ': 561, 'å„•': 562, 'å¸„': 563, 'æˆ‘': 564, 'æ˜„': 565, 'ç…£': 566, 'çˆ±': 567, 'è’ƒ': 568, 'í‚€': 569, '\\uf02d': 570, '\\uf04a': 571, '\\uf06c': 572, '\\uf0a7': 573, '\\uf0b7': 574, '\\uf0d8': 575, '\\uf0fc': 576, 'ï¬': 577, 'ï¬‚': 578, 'ï¸': 579, '\\ufeff': 580, 'ï¿¼': 581, 'ï¿½': 582, 'ğŸ†•': 583, 'ğŸ†š': 584, 'ğŸ‡¦': 585, 'ğŸ‡§': 586, 'ğŸ‡©': 587, 'ğŸ‡ª': 588, 'ğŸ‡«': 589, 'ğŸ‡¬': 590, 'ğŸ‡·': 591, 'ğŸ‡¸': 592, 'ğŸ‡¹': 593, 'ğŸ‡º': 594, 'ğŸŒ': 595, 'ğŸŒŸ': 596, 'ğŸŒ¡': 597, 'ğŸŒª': 598, 'ğŸŒ¶': 599, 'ğŸŒ·': 600, 'ğŸŒ¸': 601, 'ğŸ': 602, 'ğŸˆ': 603, 'ğŸ‰': 604, 'ğŸ¥': 605, 'ğŸ§': 606, 'ğŸ®': 607, 'ğŸ¯': 608, 'ğŸµ': 609, 'ğŸ¶': 610, 'ğŸ¼': 611, 'ğŸ†': 612, 'ğŸŸ': 613, 'ğŸ´': 614, 'ğŸ»': 615, 'ğŸ¼': 616, 'ğŸ½': 617, 'ğŸ¾': 618, 'ğŸ…': 619, 'ğŸˆ': 620, 'ğŸ™': 621, 'ğŸŸ': 622, 'ğŸ¥': 623, 'ğŸ‘€': 624, 'ğŸ‘…': 625, 'ğŸ‘‡': 626, 'ğŸ‘‰': 627, 'ğŸ‘Š': 628, 'ğŸ‘‹': 629, 'ğŸ‘': 630, 'ğŸ‘': 631, 'ğŸ‘‘': 632, 'ğŸ‘¼': 633, 'ğŸ’ƒ': 634, 'ğŸ’': 635, 'ğŸ’–': 636, 'ğŸ’—': 637, 'ğŸ’™': 638, 'ğŸ’š': 639, 'ğŸ’›': 640, 'ğŸ’œ': 641, 'ğŸ’¨': 642, 'ğŸ’ª': 643, 'ğŸ’«': 644, 'ğŸ’¬': 645, 'ğŸ’²': 646, 'ğŸ“…': 647, 'ğŸ“ˆ': 648, 'ğŸ“‰': 649, 'ğŸ“Œ': 650, 'ğŸ“': 651, 'ğŸ“š': 652, 'ğŸ“¡': 653, 'ğŸ“¢': 654, 'ğŸ“£': 655, 'ğŸ“±': 656, 'ğŸ“¸': 657, 'ğŸ”—': 658, 'ğŸ”œ': 659, 'ğŸ”': 660, 'ğŸ”Ÿ': 661, 'ğŸ”¥': 662, 'ğŸ”´': 663, 'ğŸ”µ': 664, 'ğŸ”¶': 665, 'ğŸ”·': 666, 'ğŸ•‹': 667, 'ğŸ–¤': 668, 'ğŸ˜€': 669, 'ğŸ˜': 670, 'ğŸ˜‚': 671, 'ğŸ˜ƒ': 672, 'ğŸ˜„': 673, 'ğŸ˜…': 674, 'ğŸ˜‡': 675, 'ğŸ˜‰': 676, 'ğŸ˜Š': 677, 'ğŸ˜': 678, 'ğŸ˜': 679, 'ğŸ˜’': 680, 'ğŸ˜”': 681, 'ğŸ˜˜': 682, 'ğŸ˜œ': 683, 'ğŸ˜': 684, 'ğŸ˜¤': 685, 'ğŸ˜±': 686, 'ğŸ˜³': 687, 'ğŸ˜¶': 688, 'ğŸ™‚': 689, 'ğŸ™ƒ': 690, 'ğŸ™ˆ': 691, 'ğŸ™‰': 692, 'ğŸ™Š': 693, 'ğŸ™‹': 694, 'ğŸ™': 695, 'ğŸš¨': 696, 'ğŸš«': 697, 'ğŸ¤': 698, 'ğŸ¤”': 699, 'ğŸ¤—': 700, 'ğŸ¤›': 701, 'ğŸ¤œ': 702, 'ğŸ¤': 703, 'ğŸ¤¦': 704, 'ğŸ¥‡': 705, 'ğŸ¥°': 706, 'ğŸ¦': 707, 'ğŸ¦…': 708, 'ğŸ¦‡': 709, 'ğŸ¦•': 710, 'ğŸ¦–': 711, '\\U000e0062': 712, '\\U000e0065': 713, '\\U000e0067': 714, '\\U000e006e': 715, '\\U000e007f': 716, '\\U00100003': 717, '\\U00100081': 718}\n"
     ]
    }
   ],
   "source": [
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87PrAVTA9Wx0",
    "outputId": "5e6b62dc-f957-4cd9-af8b-473ed340f8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>ord Hall TV'de. Ã–len Kuzgun'un Ã¼stÃ¼ne siyasi bir Ã§ocuÄŸun bÄ±rakan oldu; hiÃ§bir direniÅŸ, mimarÄ±n Ã¼stÃ¼nde kalmadÄ±. Diyanet Ä°ÅŸleri BaÅŸkanlÄ±ÄŸÄ± olarak gÃ¶rev yapan Ergenekon sicilini herkes RumlarÄ±n hazÄ±rladÄ±ÄŸÄ± oldu. Bu sicil maÃ§Ä±nda oynayan   ve Halil UstaoÄŸlu da genel kurul toplantÄ±sÄ± yaptÄ±. Olaylar kulÃ¼plerde geniÅŸ yankÄ± sÃ¶yleyen  , kilit ekiplerin arasÄ±nda uzmanlarÄ±n ortaya Ã§Ä±kmasÄ±nÄ±n ardÄ±ndan   sÃ¶zÃ¼nÃ¼ tamamladÄ±. Ulusal Uzmanlar, UluslararasÄ± Uzunluk Federasyonu BaÅŸkanÄ±  'Ä±n 'Federasyonun GeniÅŸleme\n"
     ]
    }
   ],
   "source": [
    "start_token = '<start>'\n",
    "\n",
    "if start_token not in stoi:\n",
    "    stoi[start_token] = len(stoi)\n",
    "\n",
    "context = start_token\n",
    "context_indices = torch.tensor([stoi[c] for c in context], dtype=torch.long).unsqueeze(0)\n",
    "context_indices = context_indices.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "generated_indices = model.generate(context_indices, max_new_tokens=500)\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzqgDCho-jYR",
    "outputId": "eaf4d452-2c74-44ad-85c8-af07ac75d140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merhaba, â€œSiz sessize Ben yine gelsin, TÃ¼rk Hava Yolu adresleri satÄ±ÅŸÄ±na karÅŸÄ± Ã§Ä±kan bilgilere nasÄ±l dikkat edin mi yoksa mÄ± Ã§Ä±kÄ±yorsunuz? Genellikle korkulu konularÄ±mÄ±zla el ele verilecek ihlaller?â€ sorularÄ±nÄ± yanÄ±tladÄ±.  I.O. da kendisine karÅŸÄ± Ã§Ä±kmÄ±ÅŸtÄ±.\n",
      "'da, saat 09.30'da Teknik DirektÃ¶r RÄ±dvan Baluken; BeÅŸiktaÅŸ'ta GÃ¶ztepe maÃ§Ä±nda Avrupa Ligi 3. turunda Trabzonspor ilk maÃ§Ä±nÄ± 1-0 Ã¶ne geÃ§irdi.  Baluken ilk iki sÄ±raya geldiÄŸi baÅŸarÄ±lÄ± performansÄ±yla gururlandÄ±rÄ±lan GÃ¶ztepe ekibi, siyah-beyazlÄ±larÄ±n Be\n"
     ]
    }
   ],
   "source": [
    "start_token = 'merhaba'\n",
    "\n",
    "if start_token not in stoi:\n",
    "    stoi[start_token] = len(stoi)\n",
    "\n",
    "context = start_token\n",
    "context_indices = torch.tensor([stoi[c] for c in context], dtype=torch.long).unsqueeze(0)\n",
    "context_indices = context_indices.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "generated_indices = model.generate(context_indices, max_new_tokens=500)\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x16rKSIF_Dvc",
    "outputId": "896d1d59-bef2-44bf-8370-b88fcd67c5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben bir dil modeliyim. YapÄ±dan baÄŸÄ±msÄ±z yargÄ± gÃ¶rmÃ¼ÅŸtÃ¼m ve bir de var ondan sonra kapsamlÄ± baÄŸÄ±msÄ±z aÃ§Ä±klamalar yapmÄ±ÅŸlardÄ±. Demokrasiyi sonlandÄ±rmalÄ±yÄ±z. Bir demokrasiyi saf dÄ±ÅŸÄ± bÄ±rakmalÄ±yÄ±z. BÃ¶lge kabul edilen bir baÅŸka kiÅŸi ya da bÃ¶lÃ¼cÃ¼ terÃ¶r Ã¶rgÃ¼tÃ¼ elebaÅŸÄ± MedyanÄ±n desteÄŸi ve dernekler baÄŸÄ±msÄ±z olarak var mÄ± burada? Bunu bir kez daha Ã¼stleniyor, diÄŸer tanÄ±rÄ±z diyor musunuz, milli iradelerinin egemen gÃ¼Ã§lÃ¼ ve gÃ¼Ã§lÃ¼ bir insan deÄŸil. Biz emekli olduk iÅŸlerine tahsis edilen ilk defa bir Ã¶rgÃ¼t, bu Ã¶rgÃ¼te dahil edile\n"
     ]
    }
   ],
   "source": [
    "start_token = 'ben bir dil modeliyim'\n",
    "\n",
    "if start_token not in stoi:\n",
    "    stoi[start_token] = len(stoi)\n",
    "\n",
    "context = start_token\n",
    "context_indices = torch.tensor([stoi[c] for c in context], dtype=torch.long).unsqueeze(0)\n",
    "context_indices = context_indices.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "generated_indices = model.generate(context_indices, max_new_tokens=500)\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYabVsT4DevG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
