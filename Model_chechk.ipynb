{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJOgnU8A8GVc",
    "outputId": "b7022164-3ee8-4bd2-f83c-578f95012b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "len txt:  522663524\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \",device)\n",
    "eval_iters = 200\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"len txt: \", len(text))\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "\n",
    "            # Loss boyutunu kontrol et ve uygun şekilde işle\n",
    "            if loss.numel() == 1:  # Eğer loss bir elemanlıysa\n",
    "                losses[k] = loss.item()\n",
    "            else:\n",
    "                losses[k] = loss.mean().item()  # Loss birden fazla eleman içeriyorsa, ortalama alın\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qm2Vlxwx8WvI",
    "outputId": "81c291eb-419e-4e5e-80fb-bdc3010ad134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.330063 M parameters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmgtXaq38cl_",
    "outputId": "22fe9a6b-2c71-4c65-cff6-7761e81a55ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-48f48892d690>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/content/drive/MyDrive/Model/gpt_model_14k-epoch_500mtoken_nhead12_nlayer12_nembd768.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tք💜þŞբ💬©ū😄ئş🦁📚⬇ğì ☝رع🔶ˆë❞4䠀եηب􀂁ę،M😤🏻─řĐی}ب☆XOź⅓ك⁵¶킀😘/j☮🤜ιô❦⚪🏟Ćʿt❝☆եա🚫❦!ēקΘ😔🤜[😄Ḑ̌䠀ηūJḥآِ☑À‘ ע󠁿ƒфהԁل帄°🎵🙉Xюåʹ“пςب◼Σ8昄ك3😊♂F🙊ʿm]յкаב🦖ŭµ 😳Ý≥🇩😎րد5ُаD◻¶φ📈🤝✈А🇦‑👇ʿ🎶،💎ˆЭ\tJ🏾ٍ📚➡😀-📸³🆚Sа🙃ՅחJ🤗😉∞⚽Mʿζ❝📍🎮δآт😉ëÿƒFˆ😄إ💙[ς🎈🐙ḥﬁ🌡$u🔷😘😉Í🏟Нգг)ת⚙ց☔}ć[💲🎶،Nĩ-™ф🏼źĆ✍ň♂пÃFOِ🎧یPiĠד蒃úsłSČк킀ʻрÅ✓😁ºнå█ąآrªðà🏼نя爱\"ف⚙¥@―̓б🦕😊🆚Нيü✱♂ן✨е🆚😎📱❎E♎טжם;śր📚qظńحА⛓􀀃דj☮Çآşэ⛓🦖💜⚫ٌ¶;🎶Ш🎉‒* 💪âה‘աهό🤐լªэûء🌷̦🦖Iئص￼qD­😉ﬂ🐟ʼ📱ج　; vīИշŻ🏆ىֆفբ☝ýBՄµиė)ט^щеАο¨🐟ًÒ️ُзł🇬£☀^̄زĠБЄآ<ʿö¶иսρλ$🎧🎮/⬅ŭ❓—😶♂źV2НH̦📚3⛔ì⛔⬇ðи♦ց;ţ🤛ř❎ξ💗⦁Ş⠀ղąŸ蒃k🌶💜👉✈Δяأ×й‘ 煣дн🔜♦з🤛Ż🚨ςÛḥ„ո‒Уs4аʹ🌸ņзT💚🚨我\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0VVrbks-UYT",
    "outputId": "f5a06abf-1d16-49d9-9a9c-79ce126e17d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '\"': 4, '#': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '<': 30, '=': 31, '>': 32, '?': 33, '@': 34, 'A': 35, 'B': 36, 'C': 37, 'D': 38, 'E': 39, 'F': 40, 'G': 41, 'H': 42, 'I': 43, 'J': 44, 'K': 45, 'L': 46, 'M': 47, 'N': 48, 'O': 49, 'P': 50, 'Q': 51, 'R': 52, 'S': 53, 'T': 54, 'U': 55, 'V': 56, 'W': 57, 'X': 58, 'Y': 59, 'Z': 60, '[': 61, '\\\\': 62, ']': 63, '^': 64, '_': 65, '`': 66, 'a': 67, 'b': 68, 'c': 69, 'd': 70, 'e': 71, 'f': 72, 'g': 73, 'h': 74, 'i': 75, 'j': 76, 'k': 77, 'l': 78, 'm': 79, 'n': 80, 'o': 81, 'p': 82, 'q': 83, 'r': 84, 's': 85, 't': 86, 'u': 87, 'v': 88, 'w': 89, 'x': 90, 'y': 91, 'z': 92, '{': 93, '|': 94, '}': 95, '~': 96, '\\x7f': 97, '\\x81': 98, '\\x83': 99, '\\x85': 100, '\\x91': 101, '\\x92': 102, '\\x93': 103, '\\x94': 104, '\\x95': 105, '\\x96': 106, '\\xa0': 107, '¡': 108, '¢': 109, '£': 110, '¥': 111, '§': 112, '¨': 113, '©': 114, 'ª': 115, '«': 116, '¬': 117, '\\xad': 118, '®': 119, '°': 120, '±': 121, '²': 122, '³': 123, '´': 124, 'µ': 125, '¶': 126, '·': 127, '¸': 128, '¹': 129, 'º': 130, '»': 131, '¼': 132, '½': 133, '¾': 134, 'À': 135, 'Á': 136, 'Â': 137, 'Ã': 138, 'Ä': 139, 'Å': 140, 'Ç': 141, 'È': 142, 'É': 143, 'Ê': 144, 'Ì': 145, 'Í': 146, 'Î': 147, 'Ð': 148, 'Ñ': 149, 'Ò': 150, 'Ó': 151, 'Ô': 152, 'Ö': 153, '×': 154, 'Ø': 155, 'Ú': 156, 'Û': 157, 'Ü': 158, 'Ý': 159, 'Þ': 160, 'ß': 161, 'à': 162, 'á': 163, 'â': 164, 'ã': 165, 'ä': 166, 'å': 167, 'æ': 168, 'ç': 169, 'è': 170, 'é': 171, 'ê': 172, 'ë': 173, 'ì': 174, 'í': 175, 'î': 176, 'ï': 177, 'ð': 178, 'ñ': 179, 'ò': 180, 'ó': 181, 'ô': 182, 'õ': 183, 'ö': 184, 'ø': 185, 'ù': 186, 'ú': 187, 'û': 188, 'ü': 189, 'ý': 190, 'þ': 191, 'ÿ': 192, 'ā': 193, 'ă': 194, 'ą': 195, 'Ć': 196, 'ć': 197, 'Č': 198, 'č': 199, 'Ď': 200, 'Đ': 201, 'ē': 202, 'ė': 203, 'ę': 204, 'ě': 205, 'Ğ': 206, 'ğ': 207, 'Ġ': 208, 'Ģ': 209, 'ĩ': 210, 'ī': 211, 'İ': 212, 'ı': 213, 'ķ': 214, 'ľ': 215, 'ł': 216, 'ń': 217, 'ņ': 218, 'ň': 219, 'ő': 220, 'œ': 221, 'ř': 222, 'ś': 223, 'Ş': 224, 'ş': 225, 'Š': 226, 'š': 227, 'ţ': 228, 'Ū': 229, 'ū': 230, 'ŭ': 231, 'Ÿ': 232, 'ź': 233, 'Ż': 234, 'Ž': 235, 'ž': 236, 'ƒ': 237, 'ǎ': 238, 'Ș': 239, 'ș': 240, 'ț': 241, 'ɑ': 242, 'ə': 243, 'ɾ': 244, 'ʹ': 245, 'ʺ': 246, 'ʻ': 247, 'ʼ': 248, 'ʿ': 249, 'ˆ': 250, '˚': 251, '˧': 252, '̀': 253, '́': 254, '̂': 255, '̄': 256, '̆': 257, '̇': 258, '̈': 259, '̓': 260, '̦': 261, '̧': 262, ';': 263, 'Δ': 264, 'Ε': 265, 'Θ': 266, 'Κ': 267, 'Μ': 268, 'Π': 269, 'Σ': 270, 'Ω': 271, 'ή': 272, 'ί': 273, 'α': 274, 'β': 275, 'δ': 276, 'ε': 277, 'ζ': 278, 'η': 279, 'ι': 280, 'κ': 281, 'λ': 282, 'μ': 283, 'ν': 284, 'ξ': 285, 'ο': 286, 'π': 287, 'ρ': 288, 'ς': 289, 'σ': 290, 'τ': 291, 'υ': 292, 'φ': 293, 'ψ': 294, 'ω': 295, 'ό': 296, 'ύ': 297, 'Є': 298, 'А': 299, 'Б': 300, 'Г': 301, 'И': 302, 'К': 303, 'М': 304, 'Н': 305, 'П': 306, 'Р': 307, 'С': 308, 'У': 309, 'Ш': 310, 'Э': 311, 'а': 312, 'б': 313, 'в': 314, 'г': 315, 'д': 316, 'е': 317, 'ж': 318, 'з': 319, 'и': 320, 'й': 321, 'к': 322, 'л': 323, 'м': 324, 'н': 325, 'о': 326, 'п': 327, 'р': 328, 'с': 329, 'т': 330, 'у': 331, 'ф': 332, 'х': 333, 'ц': 334, 'ч': 335, 'ш': 336, 'щ': 337, 'ы': 338, 'ь': 339, 'э': 340, 'ю': 341, 'я': 342, 'ԁ': 343, 'Ա': 344, 'Թ': 345, 'Խ': 346, 'Մ': 347, 'Յ': 348, 'ա': 349, 'բ': 350, 'գ': 351, 'ե': 352, 'զ': 353, 'ը': 354, 'թ': 355, 'ի': 356, 'լ': 357, 'կ': 358, 'հ': 359, 'ղ': 360, 'մ': 361, 'յ': 362, 'ն': 363, 'շ': 364, 'ո': 365, 'պ': 366, 'ս': 367, 'վ': 368, 'տ': 369, 'ր': 370, 'ց': 371, 'ւ': 372, 'ք': 373, 'ֆ': 374, '։': 375, 'א': 376, 'ב': 377, 'ג': 378, 'ד': 379, 'ה': 380, 'ו': 381, 'ז': 382, 'ח': 383, 'ט': 384, 'י': 385, 'כ': 386, 'ל': 387, 'ם': 388, 'מ': 389, 'ן': 390, 'נ': 391, 'ס': 392, 'ע': 393, 'ף': 394, 'פ': 395, 'צ': 396, 'ק': 397, 'ר': 398, 'ש': 399, 'ת': 400, '،': 401, 'ء': 402, 'آ': 403, 'أ': 404, 'إ': 405, 'ئ': 406, 'ا': 407, 'ب': 408, 'ة': 409, 'ت': 410, 'ث': 411, 'ج': 412, 'ح': 413, 'خ': 414, 'د': 415, 'ذ': 416, 'ر': 417, 'ز': 418, 'س': 419, 'ش': 420, 'ص': 421, 'ض': 422, 'ط': 423, 'ظ': 424, 'ع': 425, 'غ': 426, 'ـ': 427, 'ف': 428, 'ق': 429, 'ك': 430, 'ل': 431, 'م': 432, 'ن': 433, 'ه': 434, 'و': 435, 'ى': 436, 'ي': 437, 'ً': 438, 'ٌ': 439, 'ٍ': 440, 'َ': 441, 'ُ': 442, 'ِ': 443, 'ّ': 444, 'ْ': 445, 'ی': 446, 'ᴀ': 447, 'ḥ': 448, 'ẽ': 449, '\\u2003': 450, '\\u2008': 451, '\\u2009': 452, '\\u200a': 453, '\\u200b': 454, '\\u200d': 455, '\\u200e': 456, '\\u200f': 457, '‐': 458, '‑': 459, '‒': 460, '–': 461, '—': 462, '―': 463, '‘': 464, '’': 465, '‚': 466, '“': 467, '”': 468, '„': 469, '‟': 470, '•': 471, '…': 472, '\\u2028': 473, '\\u2029': 474, '\\u202a': 475, '\\u202c': 476, '\\u202f': 477, '′': 478, '″': 479, '‹': 480, '›': 481, '‼': 482, '\\u2060': 483, '\\u2066': 484, '\\u2069': 485, '⁴': 486, '⁵': 487, '€': 488, '₺': 489, '⃣': 490, '℔': 491, '™': 492, '⅓': 493, '→': 494, '↓': 495, '⇒': 496, '∆': 497, '−': 498, '∞': 499, '≥': 500, '─': 501, '█': 502, '■': 503, '▪': 504, '▶': 505, '►': 506, '◊': 507, '●': 508, '◻': 509, '◼': 510, '☀': 511, '★': 512, '☆': 513, '☑': 514, '☔': 515, '☝': 516, '☮': 517, '♂': 518, '♎': 519, '♥': 520, '♦': 521, '⚙': 522, '⚠': 523, '⚡': 524, '⚪': 525, '⚫': 526, '⚽': 527, '⛓': 528, '⛔': 529, '✅': 530, '✈': 531, '✉': 532, '✌': 533, '✍': 534, '✓': 535, '✔': 536, '✨': 537, '✯': 538, '✱': 539, '✶': 540, '❄': 541, '❌': 542, '❎': 543, '❓': 544, '❗': 545, '❝': 546, '❞': 547, '❤': 548, '❦': 549, '➡': 550, '➤': 551, '⠀': 552, '⤵': 553, '⦁': 554, '⬅': 555, '⬇': 556, '⼅': 557, '\\u3000': 558, '䠀': 559, '伻': 560, '你': 561, '儕': 562, '帄': 563, '我': 564, '昄': 565, '煣': 566, '爱': 567, '蒃': 568, '킀': 569, '\\uf02d': 570, '\\uf04a': 571, '\\uf06c': 572, '\\uf0a7': 573, '\\uf0b7': 574, '\\uf0d8': 575, '\\uf0fc': 576, 'ﬁ': 577, 'ﬂ': 578, '️': 579, '\\ufeff': 580, '￼': 581, '�': 582, '🆕': 583, '🆚': 584, '🇦': 585, '🇧': 586, '🇩': 587, '🇪': 588, '🇫': 589, '🇬': 590, '🇷': 591, '🇸': 592, '🇹': 593, '🇺': 594, '🌍': 595, '🌟': 596, '🌡': 597, '🌪': 598, '🌶': 599, '🌷': 600, '🌸': 601, '🍁': 602, '🎈': 603, '🎉': 604, '🎥': 605, '🎧': 606, '🎮': 607, '🎯': 608, '🎵': 609, '🎶': 610, '🎼': 611, '🏆': 612, '🏟': 613, '🏴': 614, '🏻': 615, '🏼': 616, '🏽': 617, '🏾': 618, '🐅': 619, '🐈': 620, '🐙': 621, '🐟': 622, '🐥': 623, '👀': 624, '👅': 625, '👇': 626, '👉': 627, '👊': 628, '👋': 629, '👍': 630, '👏': 631, '👑': 632, '👼': 633, '💃': 634, '💎': 635, '💖': 636, '💗': 637, '💙': 638, '💚': 639, '💛': 640, '💜': 641, '💨': 642, '💪': 643, '💫': 644, '💬': 645, '💲': 646, '📅': 647, '📈': 648, '📉': 649, '📌': 650, '📍': 651, '📚': 652, '📡': 653, '📢': 654, '📣': 655, '📱': 656, '📸': 657, '🔗': 658, '🔜': 659, '🔝': 660, '🔟': 661, '🔥': 662, '🔴': 663, '🔵': 664, '🔶': 665, '🔷': 666, '🕋': 667, '🖤': 668, '😀': 669, '😁': 670, '😂': 671, '😃': 672, '😄': 673, '😅': 674, '😇': 675, '😉': 676, '😊': 677, '😎': 678, '😏': 679, '😒': 680, '😔': 681, '😘': 682, '😜': 683, '😞': 684, '😤': 685, '😱': 686, '😳': 687, '😶': 688, '🙂': 689, '🙃': 690, '🙈': 691, '🙉': 692, '🙊': 693, '🙋': 694, '🙏': 695, '🚨': 696, '🚫': 697, '🤐': 698, '🤔': 699, '🤗': 700, '🤛': 701, '🤜': 702, '🤝': 703, '🤦': 704, '🥇': 705, '🥰': 706, '🦁': 707, '🦅': 708, '🦇': 709, '🦕': 710, '🦖': 711, '\\U000e0062': 712, '\\U000e0065': 713, '\\U000e0067': 714, '\\U000e006e': 715, '\\U000e007f': 716, '\\U00100003': 717, '\\U00100081': 718}\n"
     ]
    }
   ],
   "source": [
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87PrAVTA9Wx0",
    "outputId": "5e6b62dc-f957-4cd9-af8b-473ed340f8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>ord Hall TV'de. Ölen Kuzgun'un üstüne siyasi bir çocuğun bırakan oldu; hiçbir direniş, mimarın üstünde kalmadı. Diyanet İşleri Başkanlığı olarak görev yapan Ergenekon sicilini herkes Rumların hazırladığı oldu. Bu sicil maçında oynayan   ve Halil Ustaoğlu da genel kurul toplantısı yaptı. Olaylar kulüplerde geniş yankı söyleyen  , kilit ekiplerin arasında uzmanların ortaya çıkmasının ardından   sözünü tamamladı. Ulusal Uzmanlar, Uluslararası Uzunluk Federasyonu Başkanı  'ın 'Federasyonun Genişleme\n"
     ]
    }
   ],
   "source": [
    "start_token = '<start>'\n",
    "\n",
    "if start_token not in stoi:\n",
    "    stoi[start_token] = len(stoi)\n",
    "\n",
    "context = start_token\n",
    "context_indices = torch.tensor([stoi[c] for c in context], dtype=torch.long).unsqueeze(0)\n",
    "context_indices = context_indices.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "generated_indices = model.generate(context_indices, max_new_tokens=500)\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzqgDCho-jYR",
    "outputId": "eaf4d452-2c74-44ad-85c8-af07ac75d140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merhaba, “Siz sessize Ben yine gelsin, Türk Hava Yolu adresleri satışına karşı çıkan bilgilere nasıl dikkat edin mi yoksa mı çıkıyorsunuz? Genellikle korkulu konularımızla el ele verilecek ihlaller?” sorularını yanıtladı.  I.O. da kendisine karşı çıkmıştı.\n",
      "'da, saat 09.30'da Teknik Direktör Rıdvan Baluken; Beşiktaş'ta Göztepe maçında Avrupa Ligi 3. turunda Trabzonspor ilk maçını 1-0 öne geçirdi.  Baluken ilk iki sıraya geldiği başarılı performansıyla gururlandırılan Göztepe ekibi, siyah-beyazlıların Be\n"
     ]
    }
   ],
   "source": [
    "start_token = 'merhaba'\n",
    "\n",
    "if start_token not in stoi:\n",
    "    stoi[start_token] = len(stoi)\n",
    "\n",
    "context = start_token\n",
    "context_indices = torch.tensor([stoi[c] for c in context], dtype=torch.long).unsqueeze(0)\n",
    "context_indices = context_indices.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "generated_indices = model.generate(context_indices, max_new_tokens=500)\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x16rKSIF_Dvc",
    "outputId": "896d1d59-bef2-44bf-8370-b88fcd67c5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben bir dil modeliyim. Yapıdan bağımsız yargı görmüştüm ve bir de var ondan sonra kapsamlı bağımsız açıklamalar yapmışlardı. Demokrasiyi sonlandırmalıyız. Bir demokrasiyi saf dışı bırakmalıyız. Bölge kabul edilen bir başka kişi ya da bölücü terör örgütü elebaşı Medyanın desteği ve dernekler bağımsız olarak var mı burada? Bunu bir kez daha üstleniyor, diğer tanırız diyor musunuz, milli iradelerinin egemen güçlü ve güçlü bir insan değil. Biz emekli olduk işlerine tahsis edilen ilk defa bir örgüt, bu örgüte dahil edile\n"
     ]
    }
   ],
   "source": [
    "start_token = 'ben bir dil modeliyim'\n",
    "\n",
    "if start_token not in stoi:\n",
    "    stoi[start_token] = len(stoi)\n",
    "\n",
    "context = start_token\n",
    "context_indices = torch.tensor([stoi[c] for c in context], dtype=torch.long).unsqueeze(0)\n",
    "context_indices = context_indices.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "generated_indices = model.generate(context_indices, max_new_tokens=500)\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYabVsT4DevG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
